{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               index                                             vector\n",
      "date                                                                   \n",
      "2003-02-19 -0.020332  [0.0, 0.0, 0.0, 0.0, 0.0, 0.04272007349308638,...\n",
      "2003-02-20 -0.020332  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2003-02-21 -0.020332  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2003-02-22 -0.020332  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2003-02-23 -0.020332  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "(6365, 2)\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_df = pd.read_pickle(\"C://Users//Howard//Documents//University HW//109-1 Introduction of Text Mining//project//dataset//data.pkl\")\n",
    "print(data_df.head())\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(data_df))\n",
    "#print(data_df.index)\n",
    "#print(data_df.dtypes)\n",
    "#print(data_df.shape)\n",
    "#print(data_df.head())\n",
    "#print(data_df.tail())\n",
    "train_x = np.vstack(data_df['vector'][:-388])\n",
    "test_x = np.vstack(data_df['vector'][-388:])\n",
    "train_y = data_df['index'][:-388].to_numpy()\n",
    "test_y = data_df['index'][-388:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 float64 float64 float64\n",
      "-0.020331753554502466 27219\n",
      "epoch = 0, loss = 0.002523093717172742\n",
      "epoch = 1, loss = 0.0025221360847353935\n",
      "epoch = 2, loss = 0.0025207630824297667\n",
      "epoch = 3, loss = 0.002519769361242652\n",
      "epoch = 4, loss = 0.002518941881135106\n",
      "epoch = 5, loss = 0.0025178082287311554\n",
      "epoch = 6, loss = 0.0025170017033815384\n",
      "epoch = 7, loss = 0.002515864558517933\n",
      "epoch = 8, loss = 0.0025151276495307684\n",
      "epoch = 9, loss = 0.0025144186802208424\n",
      "epoch = 10, loss = 0.002513660117983818\n",
      "epoch = 11, loss = 0.0025130384601652622\n",
      "epoch = 12, loss = 0.0025124100502580404\n",
      "epoch = 13, loss = 0.0025116533506661654\n",
      "epoch = 14, loss = 0.002511244034394622\n",
      "epoch = 15, loss = 0.002510631922632456\n",
      "epoch = 16, loss = 0.002510164398699999\n",
      "epoch = 17, loss = 0.0025095839519053698\n",
      "epoch = 18, loss = 0.002509244717657566\n",
      "epoch = 19, loss = 0.002508749021217227\n",
      "epoch = 20, loss = 0.002508427482098341\n",
      "epoch = 21, loss = 0.002507914090529084\n",
      "epoch = 22, loss = 0.002507571130990982\n",
      "epoch = 23, loss = 0.0025072444695979357\n",
      "epoch = 24, loss = 0.0025068677496165037\n",
      "epoch = 25, loss = 0.0025068148970603943\n",
      "epoch = 26, loss = 0.0025064460933208466\n",
      "epoch = 27, loss = 0.0025062309578061104\n",
      "epoch = 28, loss = 0.002505821641534567\n",
      "epoch = 29, loss = 0.0025055846199393272\n",
      "epoch = 30, loss = 0.0025054244324564934\n",
      "epoch = 31, loss = 0.002505197189748287\n",
      "epoch = 32, loss = 0.0025050032418221235\n",
      "epoch = 33, loss = 0.002504789037629962\n",
      "epoch = 34, loss = 0.002504499163478613\n",
      "epoch = 35, loss = 0.0025044723879545927\n",
      "epoch = 36, loss = 0.0025043445639312267\n",
      "epoch = 37, loss = 0.0025041948538273573\n",
      "epoch = 38, loss = 0.0025040486361831427\n",
      "epoch = 39, loss = 0.002503944095224142\n",
      "epoch = 40, loss = 0.0025037953164428473\n",
      "epoch = 41, loss = 0.0025036437436938286\n",
      "epoch = 42, loss = 0.002503451658412814\n",
      "epoch = 43, loss = 0.0025033890269696712\n",
      "epoch = 44, loss = 0.00250326213426888\n",
      "epoch = 45, loss = 0.0025032705161720514\n",
      "epoch = 46, loss = 0.0025031608529388905\n",
      "epoch = 47, loss = 0.0025031042750924826\n",
      "epoch = 48, loss = 0.0025030188262462616\n",
      "epoch = 49, loss = 0.002502929186448455\n",
      "epoch = 50, loss = 0.0025028407108038664\n",
      "epoch = 51, loss = 0.002502763643860817\n",
      "epoch = 52, loss = 0.0025027701631188393\n",
      "epoch = 53, loss = 0.0025026481598615646\n",
      "epoch = 54, loss = 0.0025026381481438875\n",
      "epoch = 55, loss = 0.002502588788047433\n",
      "epoch = 56, loss = 0.002502557821571827\n",
      "epoch = 57, loss = 0.002502522198483348\n",
      "epoch = 58, loss = 0.0025024241767823696\n",
      "epoch = 59, loss = 0.002502384828403592\n",
      "epoch = 60, loss = 0.0025023429188877344\n",
      "epoch = 61, loss = 0.0025023191701620817\n",
      "epoch = 62, loss = 0.0025022246409207582\n",
      "epoch = 63, loss = 0.0025022344198077917\n",
      "epoch = 64, loss = 0.002502219984307885\n",
      "epoch = 65, loss = 0.0025021841283887625\n",
      "epoch = 66, loss = 0.0025021396577358246\n",
      "epoch = 67, loss = 0.0025021126493811607\n",
      "epoch = 68, loss = 0.0025021289475262165\n",
      "epoch = 69, loss = 0.0025020737666636705\n",
      "epoch = 70, loss = 0.0025020453613251448\n",
      "epoch = 71, loss = 0.002502071438357234\n",
      "epoch = 72, loss = 0.0025020516477525234\n",
      "epoch = 73, loss = 0.0025019929744303226\n",
      "epoch = 74, loss = 0.0025019627064466476\n",
      "epoch = 75, loss = 0.002501984126865864\n",
      "epoch = 76, loss = 0.0025019512977451086\n",
      "epoch = 77, loss = 0.002501950366422534\n",
      "epoch = 78, loss = 0.002501928247511387\n",
      "epoch = 79, loss = 0.0025019205641001463\n",
      "epoch = 80, loss = 0.002501901937648654\n",
      "epoch = 81, loss = 0.00250189658254385\n",
      "epoch = 82, loss = 0.002501867711544037\n",
      "epoch = 83, loss = 0.002501854207366705\n",
      "epoch = 84, loss = 0.002501842798665166\n",
      "epoch = 85, loss = 0.002501862356439233\n",
      "epoch = 86, loss = 0.002501849550753832\n",
      "epoch = 87, loss = 0.002501835348084569\n",
      "epoch = 88, loss = 0.0025018546730279922\n",
      "epoch = 89, loss = 0.0025018462911248207\n",
      "epoch = 90, loss = 0.002501858863979578\n",
      "epoch = 91, loss = 0.0025018115993589163\n",
      "epoch = 92, loss = 0.0025018262676894665\n",
      "epoch = 93, loss = 0.0025017843581736088\n",
      "epoch = 94, loss = 0.0025017964653670788\n",
      "epoch = 95, loss = 0.002501795766875148\n",
      "epoch = 96, loss = 0.002501789014786482\n",
      "epoch = 97, loss = 0.002501801820471883\n",
      "epoch = 98, loss = 0.0025017790030688047\n",
      "epoch = 99, loss = 0.0025017582811415195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0027],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0027],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0027],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0027],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0025],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026],\n",
       "        [-0.0026]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_x.dtype, train_y.dtype, test_x.dtype, test_y.dtype)\n",
    "\n",
    "np_data=data_df.to_numpy()\n",
    "print(np_data[0][0], len(np_data[0][1]))\n",
    "\n",
    "class dnn(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(dnn, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_feature, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.10),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "n_feature = len(np_data[0][1])\n",
    "n_hidden = 1000\n",
    "n_out = 1\n",
    "learningRate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model = dnn(n_feature, n_hidden, n_out)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "#train\n",
    "loss_all = []\n",
    "for t in range(100):\n",
    "    loss_sum = 0\n",
    "    #for i in range(0,len(train_x), batch_size):\n",
    "    x = Variable(torch.from_numpy(train_x).float())\n",
    "    y = Variable(torch.from_numpy(train_y).float())\n",
    "    #x = torch.reshape(torch.Tensor(train_x),(n_feature))\n",
    "    #print(x.size())\n",
    "    #y = torch.Tensor(train_y)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(x)\n",
    "    loss = loss_func(prediction, y)\n",
    "    loss_sum += loss.detach().numpy()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_all.append(loss)\n",
    "    print('epoch = {}, loss = {}'.format(t,loss_sum))\n",
    "    #optimizer.step()\n",
    "\n",
    "#predict\n",
    "model.eval()\n",
    "x = Variable(torch.from_numpy(test_x).float())\n",
    "pred_test = model(x)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
